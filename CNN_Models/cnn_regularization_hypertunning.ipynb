{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac525ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Loading the required libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Loading the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73efa7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287/287 [00:00<00:00, 1407.68it/s]\n",
      "100%|██████████| 354/354 [00:00<00:00, 1854.36it/s]\n",
      "100%|██████████| 286/286 [00:00<00:00, 1718.09it/s]\n",
      "100%|██████████| 403/403 [00:00<00:00, 1554.53it/s]\n",
      "100%|██████████| 347/347 [00:00<00:00, 1736.18it/s]\n",
      "100%|██████████| 91/91 [00:00<00:00, 1593.50it/s]\n",
      "100%|██████████| 70/70 [00:00<00:00, 1582.64it/s]\n",
      "100%|██████████| 82/82 [00:00<00:00, 1706.14it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 1645.83it/s]\n",
      "100%|██████████| 108/108 [00:00<00:00, 1610.05it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 1767.10it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 1636.20it/s]\n",
      "100%|██████████| 46/46 [00:00<00:00, 1565.24it/s]\n",
      "100%|██████████| 65/65 [00:00<00:00, 1799.28it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 1713.25it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 1628.90it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 1746.14it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 1723.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6\n",
      "Class names: ['cardboard' 'glass' 'metal' 'paper' 'plastic' 'trash']\n",
      "Training set shape: (1768, 150, 150, 3)\n",
      "Validation set shape: (328, 150, 150, 3)\n",
      "Test set shape: (431, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading the dataset\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loading the dataset and preparing it for training\n",
    "train_path = '../dataset_organized/train/'\n",
    "test_path = '../dataset_organized/test/'\n",
    "val_path = '../dataset_organized/validation/'\n",
    "\n",
    "def load_images(path):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for folder in os.listdir(path):\n",
    "        files = os.listdir(path + folder)\n",
    "        for file in tqdm(files):\n",
    "            img = cv2.imread(path + folder + '/' + file)\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            X.append(img)\n",
    "            y.append(folder)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = load_images(train_path)\n",
    "X_test, y_test = load_images(test_path)\n",
    "X_val, y_val = load_images(val_path)\n",
    "\n",
    "classes_names = np.unique(y_train)\n",
    "num_classes = len(classes_names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {classes_names}\")\n",
    "\n",
    "# Encoding the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "# One hot encoding the target variable\n",
    "y_train = to_categorical(y_train_encoded)\n",
    "y_test = to_categorical(y_test_encoded)\n",
    "y_val = to_categorical(y_val_encoded)\n",
    "\n",
    "# Normalizing the images\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Enhanced Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.8, 1.2]\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f95155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/alexandre/Secondary/Ubuntu/Projeto-CAA-1/venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 16\n",
      "num_conv_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 4, 'step': 1, 'sampling': 'linear'}\n",
      "conv_1_filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "conv_1_kernel_size (Choice)\n",
      "{'default': 3, 'conditions': [], 'values': [3, 5], 'ordered': True}\n",
      "conv_1_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'elu'], 'ordered': False}\n",
      "conv_1_l2 (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 0.001, 0.01], 'ordered': True}\n",
      "conv_2_filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "conv_2_kernel_size (Choice)\n",
      "{'default': 3, 'conditions': [], 'values': [3, 5], 'ordered': True}\n",
      "conv_2_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'elu'], 'ordered': False}\n",
      "conv_2_l2 (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 0.001, 0.01], 'ordered': True}\n",
      "num_dense_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2, 'step': 1, 'sampling': 'linear'}\n",
      "dense_1_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 512, 'step': 64, 'sampling': 'linear'}\n",
      "dense_1_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'elu'], 'ordered': False}\n",
      "dense_1_l2 (Choice)\n",
      "{'default': 0.0, 'conditions': [], 'values': [0.0, 0.001, 0.01], 'ordered': True}\n",
      "dropout_1_rate (Float)\n",
      "{'default': 0.2, 'conditions': [], 'min_value': 0.2, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "optimizer (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'rmsprop', 'sgd'], 'ordered': False}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.005, 0.001, 0.0005, 0.0001], 'ordered': True}\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |2                 |num_conv_layers\n",
      "128               |128               |conv_1_filters\n",
      "3                 |3                 |conv_1_kernel_size\n",
      "elu               |elu               |conv_1_activation\n",
      "0.001             |0.001             |conv_1_l2\n",
      "256               |256               |conv_2_filters\n",
      "3                 |3                 |conv_2_kernel_size\n",
      "elu               |elu               |conv_2_activation\n",
      "0                 |0                 |conv_2_l2\n",
      "2                 |2                 |num_dense_layers\n",
      "512               |512               |dense_1_units\n",
      "elu               |elu               |dense_1_activation\n",
      "0.01              |0.01              |dense_1_l2\n",
      "0.3               |0.3               |dropout_1_rate\n",
      "rmsprop           |rmsprop           |optimizer\n",
      "0.005             |0.005             |learning_rate\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 20:45:25.991041: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 679477248 exceeds 10% of free system memory.\n",
      "2025-04-23 20:45:26.136296: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 679477248 exceeds 10% of free system memory.\n",
      "2025-04-23 20:45:27.032471: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 679477248 exceeds 10% of free system memory.\n",
      "/media/alexandre/Secondary/Ubuntu/Projeto-CAA-1/venv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 20:45:29.712425: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 679477248 exceeds 10% of free system memory.\n",
      "2025-04-23 20:45:30.809098: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 679477248 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Number of convolutional layers\n",
    "    num_conv_layers = hp.Int('num_conv_layers', min_value=2, max_value=4)\n",
    "    \n",
    "    for i in range(num_conv_layers):\n",
    "        filters = hp.Int(f'conv_{i+1}_filters', min_value=32, max_value=128, step=32)\n",
    "        kernel_size = hp.Choice(f'conv_{i+1}_kernel_size', values=[3, 5])\n",
    "        activation = hp.Choice(f'conv_{i+1}_activation', values=['relu', 'tanh'])\n",
    "        \n",
    "        if i == 0:\n",
    "            model.add(Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=(150, 150, 3)))\n",
    "        else:\n",
    "            model.add(Conv2D(filters=filters, kernel_size=kernel_size, activation=activation))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layer\n",
    "    dense_units = hp.Int('dense_units', min_value=64, max_value=256, step=64)\n",
    "    model.add(Dense(units=dense_units, activation=hp.Choice('dense_activation', values=['relu', 'tanh'])))\n",
    "    model.add(Dropout(hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
    "    else:\n",
    "        opt = SGD(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # Number of models to train per combination\n",
    "    directory='my_dir',\n",
    "    project_name='cnn_hyperparameter_tuning'\n",
    ")\n",
    "\n",
    "# Assuming you have already loaded your data into X_train, y_train, X_val, y_val\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of convolutional layers is {best_hps.get('num_conv_layers')},\n",
    "the optimal number of filters for each layer and corresponding kernel sizes and activations are:\n",
    "- Layer 1: {best_hps.get('conv_1_filters')} filters, kernel size {best_hps.get('conv_1_kernel_size')}, activation {best_hps.get('conv_1_activation')}\n",
    "- Layer 2: {best_hps.get('conv_2_filters')} filters, kernel size {best_hps.get('conv_2_kernel_size')}, activation {best_hps.get('conv_2_activation')}\n",
    "{'' if best_hps.get('num_conv_layers') < 3 else f'- Layer 3: {best_hps.get(\"conv_3_filters\")} filters, kernel size {best_hps.get(\"conv_3_kernel_size\")}, activation {best_hps.get(\"conv_3_activation\")}'}\n",
    "{'' if best_hps.get('num_conv_layers') < 4 else f'- Layer 4: {best_hps.get(\"conv_4_filters\")} filters, kernel size {best_hps.get(\"conv_4_kernel_size\")}, activation {best_hps.get(\"conv_4_activation\")}'}\n",
    "the optimal number of units in the dense layer is {best_hps.get('dense_units')},\n",
    "the optimal activation function for the dense layer is {best_hps.get('dense_activation')},\n",
    "the optimal dropout rate is {best_hps.get('dropout_rate')},\n",
    "and the optimal optimizer and learning rate are {best_hps.get('optimizer')} with a learning rate of {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build the model with the best hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Save the best model\n",
    "model.save('hypertuning_cnn_model.h5')\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ec33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "from keras.models import load_model\n",
    "model = load_model('hypertuning_cnn_model.h5')\n",
    "\n",
    "# Evaluating the model\n",
    "predictions = model.predict(X_test)\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "# f1 score, precision, recall, accuracy, confusion matrix, classification report, and ROC curve\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_curve, auc\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(np.argmax(y_test_encoded, axis=1), np.argmax(predictions, axis=1), average='weighted')\n",
    "print(\"F1 Score: \", f1)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(np.argmax(y_test_encoded, axis=1), np.argmax(predictions, axis=1), average='weighted')\n",
    "print(\"Precision: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(np.argmax(y_test_encoded, axis=1), np.argmax(predictions, axis=1), average='weighted')\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(np.argmax(y_test_encoded, axis=1), np.argmax(predictions, axis=1))\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# ROC Curve\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(6):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_encoded[:, i], predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(6):\n",
    "    plt.plot(fpr[i], tpr[i], label=classes_names[i] + ' (AUC = ' + str(roc_auc[i]) + ')')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(np.argmax(y_test_encoded, axis=1), np.argmax(predictions, axis=1), target_names=classes_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = confusion_matrix(np.argmax(y_test_encoded, axis=1), np.argmax(predictions, axis=1))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes_names, yticklabels=classes_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
